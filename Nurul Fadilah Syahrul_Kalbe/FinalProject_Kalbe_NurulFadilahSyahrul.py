# -*- coding: utf-8 -*-
"""Project-Kalbe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WeGadKYKVev-mFtWW4C2ifmCUzdnrK9E

# Import Library
"""

# import library
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# import time series library
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import warnings
warnings.filterwarnings('ignore')

"""# Load Dataset"""

# load dataset
df_customer = pd.read_csv('Customer.csv', delimiter = ';')
df_product = pd.read_csv('Product.csv', delimiter = ';')
df_store = pd.read_csv('Store.csv', delimiter = ';')
df_transaction = pd.read_csv('Transaction.csv', delimiter = ';')

"""# Data Cleaning"""

# data cleaning
# head data
print(df_customer.head())
print(df_product.head())
print(df_store.head())
print(df_transaction.head())

# info data
print(df_customer.info())
print(df_product.info())
print(df_store.info())
print(df_transaction.info())

# change data type
df_customer['Income'] = df_customer['Income'].str.replace(',', '.').astype('float')
df_store['Latitude'] = df_store['Latitude'].str.replace(',', '.').astype('float')
df_store['Longitude'] = df_store['Longitude'].str.replace(',', '.').astype('float')
df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

# merge data
df_merge = pd.merge(df_customer, df_transaction, on = 'CustomerID')
df_merge = pd.merge(df_merge, df_product.drop(columns = 'Price'), on = 'ProductID')
df_merge = pd.merge(df_merge, df_store, on = 'StoreID')
df_merge.head()

"""# Time Series

## Modelling
"""

# model regresi: time series
df_regresi = df_merge.groupby('Date').agg({'Qty':'sum'}).reset_index()
df_regresi.head()

fig, ax = plt.subplots(figsize=(9, 5))
sns.lineplot(x = 'Date', y = 'Qty', data = df_regresi)
plt.tight_layout()

df_regresi_index = df_regresi.set_index('Date')
regresi_decomposition = seasonal_decompose(df_regresi_index)

# Plot the decomposition results
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize = (10, 8), sharex = True)
regresi_decomposition.observed.plot(ax = ax1)
ax1.set_ylabel('Observed')
regresi_decomposition.trend.plot(ax = ax2)
ax2.set_ylabel('Trend')
regresi_decomposition.seasonal.plot(ax = ax3)
ax3.set_ylabel('Seasonal')
regresi_decomposition.resid.plot(ax = ax4)
ax4.set_ylabel('Residual')

plt.tight_layout()

# Splitting the data into training and testing sets
train_size = int(len(df_regresi) * 0.8)  # 80% data for training, 20% for testing
train_data, test_data = df_regresi.iloc[:train_size], df_regresi.iloc[train_size:]

train_data.head()

test_data.head()

train_data_index = train_data.set_index('Date')
regresi_decomposition = seasonal_decompose(train_data_index)

# Plot the decomposition results
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize = (10, 8), sharex = True)
regresi_decomposition.observed.plot(ax = ax1)
ax1.set_ylabel('Observed')
regresi_decomposition.trend.plot(ax = ax2)
ax2.set_ylabel('Trend')
regresi_decomposition.seasonal.plot(ax = ax3)
ax3.set_ylabel('Seasonal')
regresi_decomposition.resid.plot(ax = ax4)
ax4.set_ylabel('Residual')

plt.tight_layout()

# Statistical test to validate stationarity

# Ho = The data is not stationary
# Ha = The data is stationary

alpha = 0.05
adfuller_pvalue = adfuller(train_data['Qty'])[1]

if adfuller_pvalue <= alpha:
    print('Reject Ho. The data is stationary')
    print(adfuller_pvalue)
else:
    print('Fail to reject Ho. The data is not stationary')
    print(adfuller_pvalue)

"""Uji ADF di atas menunjukkan bahwa data sudah stasioner. Akan tetapi, plot time series menunjukkan datanya belum stasioner secara musiman. Sehingga, akan dilakukan differencing tiap lag musimannya, yaitu lag 7."""

diff_mus7 = train_data['Qty'].diff(periods = 7)
diff_mus7

# Statistical test to validate stationarity

# Ho = The data is not stationary
# Ha = The data is stationary

alpha = 0.05
adfuller_pvalue = adfuller(diff_mus7.dropna())[1]

if adfuller_pvalue < alpha:
    print('Reject Ho. The data is stationary')
    print(adfuller_pvalue)
else:
    print('Fail to reject Ho. The data is not stationary')
    print(adfuller_pvalue)

# subplot
fig, ax = plt.subplots(2, 3, figsize = (15,5))

# plot the original data
ax[0, 0].plot(train_data['Qty'])
ax[0, 0].set_title('Original Data')

# plot the ACF and PACF
plot_acf(train_data['Qty'], ax = ax[0, 1])
ax[0, 1].set_title('Autocorrelation Function (ACF)')

plot_pacf(train_data['Qty'], ax = ax[0, 2])
ax[0, 2].set_title('Partial Autocorrelation Function (PACF)')

# plot the differencing data
ax[1, 0].plot(diff_mus7.dropna())
ax[1, 0].set_title('Differencing Data')

# plot the ACF and PACF
plot_acf(diff_mus7.dropna(), ax = ax[1, 1])
ax[1, 1].set_title('Autocorrelation Function (ACF)')

plot_pacf(diff_mus7.dropna(), ax = ax[1, 2])
ax[1, 2].set_title('Partial Autocorrelation Function (PACF)')

plt.tight_layout()

"""Model Seasonal ARIMA yang terbentuk adalah ARIMA (0, 0, 0) dengan seasonal (0, 1, 1, 7). Akan tetapi, kita juga akan melakukan pengecekan untuk model gabungan MA = 1 dan/atau AR = 1, sehingga model yang mungkin terbentuk adalah model seasonal (0, 1, 1, 7), (1, 1, 0, 7), atau (1, 1, 1, 7)."""

def rmse(y_actual, y_pred):
  print(f'RMSE Value: {mean_squared_error(y_actual, y_pred)**0.5}')
def rsquare(y_actual, y_pred):
  print(f'R-squared Value: {r2_score(y_actual, y_pred)}')
def eval(y_actual, y_pred):
  rmse(y_actual, y_pred)
  rsquare(y_actual, y_pred)
  print(f'MAE Value: {mean_absolute_error(y_actual, y_pred)}')

# Fit the ARIMA(0,0,0)(0,1,1,7) seasonal model
order = (0,0,0)
seasonal_order = (0, 1, 1, 7)
model = sm.tsa.SARIMAX(train_data['Qty'], order = order, seasonal_order = seasonal_order)
fit_qty = model.fit()
print(fit_qty.summary())

# ARIMA (0,0,0) Seasonal (0,1,1,7)
df_train = train_data.set_index('Date')
df_test = test_data.set_index('Date')

y_pred = fit_qty.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = fit_qty.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

eval(df_test['Qty'], y_pred_out)

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'Seasonal ARIMA Predictions')
plt.legend()

plt.tight_layout()

# Fit the ARIMA(0,0,0)(1,1,1,7) seasonal model
order = (0,0,0)
seasonal_order = (1, 1, 1, 7)
model = sm.tsa.SARIMAX(train_data['Qty'], order = order, seasonal_order = seasonal_order)
fit_qty = model.fit()
print(fit_qty.summary())

# ARIMA (0,0,0) Seasonal (1,1,1,7)
df_train = train_data.set_index('Date')
df_test = test_data.set_index('Date')

y_pred = fit_qty.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = fit_qty.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

eval(df_test['Qty'], y_pred_out)

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'Seasonal ARIMA Predictions')
plt.legend()

plt.tight_layout()

# Fit the ARIMA(0,0,0)(1,1,0,7) seasonal model
order = (0,0,0)
seasonal_order = (1, 1, 0, 7)
model = sm.tsa.SARIMAX(train_data['Qty'], order = order, seasonal_order = seasonal_order)
fit_qty = model.fit()
print(fit_qty.summary())

# ARIMA (0,0,0) Seasonal (1,1,0,7)
df_train = train_data.set_index('Date')
df_test = test_data.set_index('Date')

y_pred = fit_qty.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = fit_qty.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

eval(df_test['Qty'], y_pred_out)

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'Seasonal ARIMA Predictions')
plt.legend()

plt.tight_layout()

"""## Forecast"""

model = sm.tsa.SARIMAX(df_test['Qty'], order = (0, 0, 0), seasonal_order = (1, 1, 0, 7))
fit_qty = model.fit()
print(fit_qty.summary())

forecast_length = 31
forecast_result = fit_qty.get_forecast(forecast_length)
forecast_result_arima = forecast_result.conf_int()
forecast_result_arima['forecasted Qty'] = fit_qty.predict(start = forecast_result_arima.index[0],
                                                      end = forecast_result_arima.index[-1])
forecast_result_arima['Date'] = pd.date_range(start = '2023-01-01', end = '2023-01-31')
forecast_result_arima.set_index('Date', inplace = True)
forecast_result_arima.head()

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'Seasonal ARIMA Predictions')
plt.plot(forecast_result_arima['forecasted Qty'], color = 'green', label = 'Seasonal ARIMA Forecasted')
plt.legend()

plt.tight_layout()

"""# KMeans Clustering"""

# clustering model
df_cluster = df_merge.groupby('CustomerID').agg({'TransactionID':'count',
                                                 'Qty':'sum',
                                                 'TotalAmount':'sum'}).reset_index()
df_cluster.head()

"""## Data Normalization"""

# normalization per feature
from sklearn.preprocessing import MinMaxScaler

df_fix = df_cluster.drop('CustomerID', axis = 1)
num_fix = df_fix.columns

df_norm = MinMaxScaler().fit_transform(df_fix)
df_norm = pd.DataFrame(data = df_norm, columns = num_fix)
df_norm.head()

#check distribution
plt.figure(figsize = (15,4))
for i in range(0, len(num_fix)):
  plt.subplot(1, 3, i + 1)
  sns.distplot(df_norm[num_fix[i]], color = 'red')
  plt.xlabel(num_fix[i])
  plt.tight_layout()

"""## Data Modelling"""

# check descriptive statistics
df_norm.describe().transpose()

from sklearn.cluster import KMeans
inertia = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)
    kmeans.fit(df_norm.values)
    inertia.append(kmeans.inertia_)

sns.lineplot(x = range(1,11), y = inertia, linewidth = 4)
sns.scatterplot(x = range(1,11), y = inertia, s = 60)
plt.tight_layout()

pd.Series(inertia) - pd.Series(inertia).shift(-1)

# Distortion Score Elbow
from yellowbrick.cluster import KElbowVisualizer

# fit model
model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k = (2,11), metric = 'distortion', timings = True, locate_elbow = True)
visualizer.fit(df_norm)
visualizer.show()

# silhouette score elbow
model = KMeans(random_state = 42)
visualizer = KElbowVisualizer(model, k = (2,11), metric = 'silhouette', timings = True, locate_elbow = True)
visualizer.fit(df_norm)
visualizer.show()

# silhouette plot
from yellowbrick.cluster import SilhouetteVisualizer

for i in range(2,6):
    model = KMeans(i, random_state = 42)
    visualizer = SilhouetteVisualizer(model, colors = 'yellowbrick')
    visualizer.fit(df_norm)
    visualizer.show()

from sklearn.metrics import silhouette_score
for num_clusters in range(2,6):
    #inisialisasi kmeans
    model_clus = KMeans(n_clusters = num_clusters, max_iter = 1000, random_state = 42)
    model_clus.fit(df_norm)

    cluster_labels = model_clus.labels_

    #shilhouette score
    silhouette_avg = silhouette_score(df_norm, cluster_labels)
    print("For n_clusters = {0}, the silhouette score is {1}".format(num_clusters, silhouette_avg))

kmeans = KMeans(n_clusters = 3, max_iter = 300, random_state = 42)
kmeans.fit(df_norm.values)

#display label  to dataset
# kmeans.labels_ : df_norm
df_norm['label'] = kmeans.labels_

# display df_std
df_norm.head()

#PCA - split
X = df_norm.copy().drop(['label'], axis = 1)
Y = df_norm['label'].copy()

scaler = MinMaxScaler()
scaler.fit(X)
X_norm = scaler.transform(X)

#PCA
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X_norm)

# the attribute shows how much variance is explained by each of the three individual components
print('Explained variance ratio:', pca.explained_variance_ratio_)

# plot variance ratio cumulative sum of components
plt.plot(range(1,4), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')
plt.title('Explained Variance by Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

#PCA
pca = PCA(n_components = 2)
pca.fit(X_norm)
X_pca = pca.transform(X_norm)

pdf = pd.DataFrame(X_pca, columns = ['1', '2'])
pdf['label'] = Y
pdf.sample(5)

pdf.describe()

pdf.groupby('label').agg(['mean', 'median', 'std', 'count'])

sns.scatterplot(data = pdf, x = '1', y = '2', hue = 'label', palette='RdYlBu')
plt.show()

"""## Customer Personality Analysis"""

df_fix['cluster'] = kmeans.labels_
df_fix.groupby('cluster').agg(['count','mean', 'median', 'std'])

df_fix.describe().T

# percentage of customers per cluster
cluster_count = df_fix['cluster'].value_counts().reset_index()
cluster_count.columns = ['cluster', 'count']
cluster_count['percentage (%)'] = round((cluster_count['count']/len(df_fix))*100,2)
cluster_count = cluster_count.sort_values(by = ['cluster']).reset_index(drop = True)
cluster_count

#visualization of the percentage of customers in each cluster
fig, ax = plt.subplots(figsize = (7,4))

c = ["#d7191c","#fdae61","#2c7bb6","#abd9e9", "#ffffbf"]

bars = plt.bar(x = cluster_count['cluster'],
               height = cluster_count['percentage (%)'],
               color = c)

for bar in bars:
  height = bar.get_height()
  label_x_pos = bar.get_x() + bar.get_width() / 2
  ax.text(label_x_pos, height, s = f'{height} %', ha='center',
  va = 'bottom')

plt.title('Percentage of Customer by Cluster', fontsize = 16)
plt.xlabel('Cluster',fontsize = 12)
plt.ylabel('Percentage',fontsize = 12)
plt.style.use('tableau-colorblind10')
plt.grid(False)
plt.tight_layout()

# displays the column pattern in each cluster
cluster_med = df_norm.groupby('label').mean().reset_index()

df_melt = pd.melt(cluster_med.reset_index(),
                  id_vars='label',
                  value_vars = num_fix,
                  var_name = 'Metric',
                  value_name = 'Value')

plt.figure(figsize=(10,6))
sns.pointplot(data = df_melt, x = 'Metric', y = 'Value', hue = 'label', palette = c)
plt.title('Pattern of Customer by KMeans Clustering Model', fontsize = 14)
plt.xlabel('Metric')
plt.ylabel('Value')
plt.tight_layout()

# displays the average of the variables between clusters
def dist_list(clust):
    plt.figure(figsize = [len(clust)*4,3])
    i = 1
    for col in clust:
        ax = plt.subplot(1,len(clust),i)
        ax.vlines(df_fix[col].mean(), ymin = -0.5, ymax = 2.5, color = 'black', linestyle='--')
        g = df_fix.groupby('cluster')
        x = g[col].mean().index
        y = g[col].mean().values
        ax.barh(x,y, color = c)
        plt.title(col)
        plt.grid(False)
        i = i+1

dist_list(num_fix)

df_fix.groupby('cluster').agg(['min', 'max', 'mean']).reset_index(drop = True).T

"""<p style="text-align: center;">
Tabel - Akumulasi Hasil Nilai Fitur pada Setiap Cluster
</p>

|  Cluster  | High Value | Average Value | Low Value |
| :-------- | :--------: | :-----------: | :-------: |
| **Cluster 0** | TransactionID, Qty, TotalAmount |  |  |
| **Cluster 1** |  | TransactionID, Qty, TotalAmount |  |
| **Cluster 2** |  |  | TransactionID, Qty, TotalAmount |

### Clustering Interpretation

1. **Cluster 0 - Loyalty Customer**
    - Terdapat 111 customer (24.83%).
    - Customer di kelompok ini memiliki rata-rata transaksi yang tinggi, yaitu sekitar 15 kali transaksi, rata-rata jumlah atau kuantitas produk yang dibeli customer tinggi yaitu sekitar 58 unit produk, dan rata-rata jumlah uang yang dikeluarkan oleh customer tinggi yaitu sekitar 524K.
</p>
2. **Cluster 1 - Potential Customer**
    - Terdapat 206 customer (46.09%)
    - Customer di kelompok ini memiliki rata-rata transaksi yang sedang, yaitu sekitar 11 kali transaksi, rata-rata jumlah atau kuantitas produk yang dibeli customer sedang yaitu sekitar 41 unit produk, dan rata-rata jumlah uang yang dikeluarkan oleh customer sedang yaitu sekitar 360K.
</p>
3. **Cluster 2 - New Customer**
    - Terdapat 130 customer (29.08%)
    - Customer di kelompok ini memiliki rata-rata transaksi yang rendah, yaitu sekitar 8 kali transaksi, rata-rata jumlah atau kuantitas produk yang dibeli customer rendah yaitu sekitar 27 unit produk, dan rata-rata jumlah uang yang dikeluarkan oleh customer rendah yaitu sekitar 229K.

### Business Recommendation
1. **Cluster 0 - Loyalty Customer**: memberikan email khusus kepada customer atas keloyalitasan telah menggunakan produk dari company kami berupa ucapan terima kasih telah setia menggunakan produk kami dengan menyertakan reward voucher diskon berbelanja tanpa minimum pembelian beserta voucher gratis ongkir tanpa minimum pembelian dan dapat ditukarkan pada batas waktu tertentu.
</p>
2. **Cluster 1 - Potential Customer**: kelompok ini memiliki potensi untuk menjadi customer yang loyal menggunakan produk dari company. Hal yang dapat direkomendasikan berupa pemberian voucher diskon khusus pada produk yang sering dibeli oleh customer ini dengan voucher gratis ongkir dengan nol minimum pembelian.
</p>
3. **Cluster 2 - New Customer**: memberikan email khusus kepada customer dengan caption seperti "we miss you" agar kelompok ini bisa lebih sering berbelanja produk pada company kami dengan menyertakan reward voucher diskon berbelanja jika telah mencapai minimum pembelian yang telah ditentukan oleh company beserta voucher gratis ongkir jika mencapai minimum pembelian yang telah ditentukan oleh company juga dan dapat ditukarkan pada batas waktu tertentu.
"""